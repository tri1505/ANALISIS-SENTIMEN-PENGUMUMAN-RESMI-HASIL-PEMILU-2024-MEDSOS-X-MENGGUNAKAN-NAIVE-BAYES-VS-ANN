# -*- coding: utf-8 -*-
"""Preprocessing_Hasil pemilu_Neural Network.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ai1iO5pAwJHE2cGf8m3hhfyLjQbpkRk_
"""

!pip install Sastrawi

import pandas as pd
import re
import string
import nltk
import Sastrawi
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS

data = pd.read_csv('Semua.csv')
data

"""**CASE FOLDING**"""

def case_folding(text):
    if isinstance(text, str):
        lowercase_text = text.lower()
        return lowercase_text
    else:
        return text

data['case_folding'] = data['full_text'].apply(case_folding)
data

del(data['full_text'])
data

"""**CLEANING**"""

def remove_URL(tweet):
  url = re.compile(r'https?://\S+|www\.\S+')
  return url.sub(r'', tweet)

def remove_html(tweet):
  html = re.compile(r'<.*?>')
  return html.sub(r'', tweet)

def remove_emoji(tweet):
  emoji_pattern = re.compile("["
      u"\U0001F600-\U0001F64F"
      u"\U0001F300-\U0001F5FF"
      u"\U0001F680-\U0001F6FF"
      u"\U0001F1E0-\U0001F1FF"
                              "]+", flags=re.UNICODE)
  return emoji_pattern.sub(r'', tweet)

def remove_mentions(tweet):
  tweet = re.sub(r'@[a-zA-Z0-9_]+','', tweet)
  return tweet

def remove_hashtag(tweet):
  tweet = re.sub(r'#\w+','', tweet)
  return tweet

def remove_symbols(tweet):
  tweet = re.sub(r'[^a-zA-Z0-9\s]','', tweet) # Menghapus semua simbol
  return tweet

def remove_numbers(tweet):
  tweet = re.sub(r'\d+', '', tweet)
  return tweet

data['cleaning'] = data['case_folding'].apply(lambda x: remove_URL(x))
data['cleaning'] = data['cleaning'].apply(lambda x: remove_html(x))
data['cleaning'] = data['cleaning'].apply(lambda x: remove_emoji(x))
data['cleaning'] = data['cleaning'].apply(lambda x: remove_mentions(x))
data['cleaning'] = data['cleaning'].apply(lambda x: remove_hashtag(x))
data['cleaning'] = data['cleaning'].apply(lambda x: remove_symbols(x))
data['cleaning'] = data['cleaning'].apply(lambda x: remove_numbers(x))

data

# Fungsi penggantian kata tidak baku
def replace_taboo_words (text, kamus_tidak_baku):
    if isinstance(text, str):
        words = text.split()
        replaced_words = []
        kalimat_baku = []
        kata_diganti = []
        kata_tidak_baku_hash = []

        for word in words:
            if word in kamus_tidak_baku:
                baku_word = kamus_tidak_baku[word]
                if isinstance(baku_word, str) and all(char.isalpha() for char in baku_word):
                    replaced_words.append(baku_word)
                    kalimat_baku.append(baku_word)
                    kata_diganti.append(word)
                    kata_tidak_baku_hash.append(hash (word))
            else:
                replaced_words.append(word)
                replaced_text = ' '.join(replaced_words)
    else:
        replaced_text = ''
        kalimat_baku = []
        kata_diganti = []
        kata_tidak_baku_hash = []

    return replaced_text

# Baca kamus data tidak baku
kamus_data = pd.read_csv('normalisasi.csv')
kamus_tidak_baku = dict(zip(kamus_data['kata_tidak_baku'],kamus_data['kata_baku']))

# Terapkan fungsi penggantian kata tidak baku
data['cleaning']=data['cleaning'].apply(lambda x:replace_taboo_words(x, kamus_tidak_baku))
data

del(data['case_folding'])
data

data.to_csv('Hasil_normalisasi_NN.csv',encoding='utf8', index = False)

data = pd.read_csv('Hasil_normalisasi_NN_1500Data.csv')
data

"""**WORDCLOUD**"""

def plot_cloud (wordcloud):
    plt.figure(figsize=(10, 8))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.show()

all_words = ' '.join([tweets for tweets in data['cleaning']])

wordcloud = WordCloud (
    width=3000,
    height=2000,
    random_state=3,
    background_color='black',
    colormap='RdPu',
    collocations=False,
    stopwords=STOPWORDS
).generate(all_words)
plot_cloud (wordcloud)

!pip install pip install scikit-learn

import pandas as pd
import numpy as np

from numpy import array
from numpy import asarray
from numpy import zeros
from tensorflow.keras.preprocessing.text import one_hot, Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation, Dropout, Dense, BatchNormalization
from tensorflow.keras.layers import Flatten, GlobalMaxPooling1D, Embedding, Conv1D, LSTM
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from keras import backend as K
import gc

import matplotlib.pyplot as plt
import seaborn as sns

"""**Tokenizing**"""

text = data['cleaning'].tolist()
y = data['sentimen']
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y)

word_tokenizer = Tokenizer()
word_tokenizer.fit_on_texts(text)

vocab = len(word_tokenizer.index_word) + 1

encode_text = word_tokenizer.texts_to_sequences(text)

max_length = 100
X = pad_sequences(encode_text, maxlen=max_length, padding='post')
print(X)

"""**PEMODELAN**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

model = Sequential()
embedding_layer = Embedding(input_dim=vocab, output_dim=64 , input_length=max_length)
model.add (embedding_layer)
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
#BatchNormalization(momentum=0.80)
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(3, activation='softmax'))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
print(model.summary())

def reset_keras():
    K.clear_session()
    gc.collect()

reset_keras()

print(type(model.loss))
print(type(model.metrics))
model_history=model.fit(X_train, y_train, epochs=40,batch_size=64,validation_data=(X_test,y_test), verbose=2)

# Membuat prediksi pada set pengujian
#score = model.evaluate(X_train, y_train, verbose=1)
#print("Train Loss:", score[0])
#print("Train Accuracy:", score[1])

score2 = model.evaluate(X_test, y_test, batch_size=1)
#print("Test Loss:", score2[0])
print("Test Accuracy:", score2[1])
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_test_classes = np.argmax(y_test, axis=1)

best_epoch = np.argmax(model_history.history['val_accuracy']) + 1
best_accuracy = max(model_history.history['val_accuracy'])

print(f'Epoch dengan akurasi tertinggi: {best_epoch}')
print(f'Akurasi tertinggi pada data validasi: {best_accuracy:.4f}')

# Menghitung confusion matrix
cm = confusion_matrix(y_test_classes, y_pred_classes)

# Menampilkan confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['negatif', 'netral', 'positif'], yticklabels=['negatif', 'netral', 'positif'])
plt.xlabel('Kelas Prediksi')
plt.ylabel('Kelas Aktual')
plt.title('Confusion Matrix')
plt.show()

# Menampilkan laporan klasifikasi
print(classification_report(y_test_classes, y_pred_classes, target_names=['Negative', 'Neutral', 'Positive']))

# Teks baru yang ingin diuji
new_texts = ["Pemilu sangat bagus"]

# Preprocess teks baru
new_encode_text = word_tokenizer.texts_to_sequences(new_texts)
new_X = pad_sequences(new_encode_text, maxlen=max_length, padding='post')

# Prediksi dengan model
new_predictions = model.predict(new_X)

# Mengambil label dengan probabilitas tertinggi
predicted_label = np.argmax(new_predictions, axis=1)

# Konversi label prediksi kembali ke label asli
predicted_sentiment = label_encoder.inverse_transform(predicted_label)
print(f"Prediksi Sentimen: {predicted_sentiment[0]}")