# -*- coding: utf-8 -*-
"""Preprocessing_Hasil pemilu_Naive_Bayes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ssg1ZqhpDxdV3EnoS6h7T7LA2D-ETs07
"""

!pip install Sastrawi

import pandas as pd
import re
import string
import nltk
import Sastrawi
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from nltk.stem import PorterStemmer
from nltk.stem.snowball import SnowballStemmer

data = pd.read_csv('Semua.csv')
data

"""**CASE FOLDING**"""

def case_folding(text):
    if isinstance(text, str):
        lowercase_text = text.lower()
        return lowercase_text
    else:
        return text

data['case_folding'] = data['full_text'].apply(case_folding)
data

del(data['full_text'])
data

"""**CLEANING**"""

def remove_URL(tweet):
  url = re.compile(r'https?://\S+|www\.\S+')
  return url.sub(r'', tweet)

def remove_html(tweet):
  html = re.compile(r'<.*?>')
  return html.sub(r'', tweet)

def remove_emoji(tweet):
  emoji_pattern = re.compile("["
      u"\U0001F600-\U0001F64F"
      u"\U0001F300-\U0001F5FF"
      u"\U0001F680-\U0001F6FF"
      u"\U0001F1E0-\U0001F1FF"
                              "]+", flags=re.UNICODE)
  return emoji_pattern.sub(r'', tweet)

def remove_mentions(tweet):
  tweet = re.sub(r'@[a-zA-Z0-9_]+','', tweet)
  return tweet

def remove_hashtag(tweet):
  tweet = re.sub(r'#\w+','', tweet)
  return tweet

def remove_symbols(tweet):
  tweet = re.sub(r'[^a-zA-Z0-9\s]','', tweet) # Menghapus semua simbol
  return tweet

def remove_numbers(tweet):
  tweet = re.sub(r'\d+', '', tweet)
  return tweet

data['cleaning'] = data['case_folding'].apply(lambda x: remove_URL(x))
data['cleaning'] = data['cleaning'].apply(lambda x: remove_html(x))
data['cleaning'] = data['cleaning'].apply(lambda x: remove_emoji(x))
data['cleaning'] = data['cleaning'].apply(lambda x: remove_mentions(x))
data['cleaning'] = data['cleaning'].apply(lambda x: remove_hashtag(x))
data['cleaning'] = data['cleaning'].apply(lambda x: remove_symbols(x))
data['cleaning'] = data['cleaning'].apply(lambda x: remove_numbers(x))

data

del(data['case_folding'])
data

"""**STOPWORD**"""

# Fungsi penggantian kata tidak baku
def replace_taboo_words (text, kamus_tidak_baku):
    if isinstance(text, str):
        words = text.split()
        replaced_words = []
        kalimat_baku = []
        kata_diganti = []
        kata_tidak_baku_hash = []

        for word in words:
            if word in kamus_tidak_baku:
                baku_word = kamus_tidak_baku[word]
                if isinstance(baku_word, str) and all(char.isalpha() for char in baku_word):
                    replaced_words.append(baku_word)
                    kalimat_baku.append(baku_word)
                    kata_diganti.append(word)
                    kata_tidak_baku_hash.append(hash (word))
            else:
                replaced_words.append(word)
                replaced_text = ' '.join(replaced_words)
    else:
        replaced_text = ''
        kalimat_baku = []
        kata_diganti = []
        kata_tidak_baku_hash = []

    return replaced_text

# Baca kamus data tidak baku
kamus_data = pd.read_csv('normalisasi.csv')
kamus_tidak_baku = dict(zip(kamus_data['kata_tidak_baku'],kamus_data['kata_baku']))

# Terapkan fungsi penggantian kata tidak baku
data['cleaning']=data['cleaning'].apply(lambda x:replace_taboo_words(x, kamus_tidak_baku))

more_stop_words = []

stop_words = StopWordRemoverFactory().get_stop_words()
stop_words.extend(more_stop_words)

new_array = ArrayDictionary(stop_words)
stop_words_remover_new = StopWordRemover(new_array)

def stopword(str_text):
    str_text = stop_words_remover_new.remove(str_text)
    return str_text

data['stopword'] = data['cleaning'].apply(lambda x: stopword(x))
data

del(data['cleaning'])
data

"""**TOKENIZING**"""

def tokenize(text):
    tokens = text.split()
    return tokens

data['tokenize'] = data['stopword'].apply(tokenize)
data

del(data['stopword'])
data

"""**Stemming**"""

def stemming (text_cleaning):
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()
    do = []
    for w in text_cleaning:
        dt = stemmer.stem(w)
        do.append(dt)
    d_clean = []
    d_clean = " ".join(do)
    print(d_clean)
    return d_clean

data['stemming'] = data['tokenize'].apply(stemming)
data

# Terapkan fungsi penggantian kata tidak baku
data['stemming']=data['stemming'].apply(lambda x:replace_taboo_words(x, kamus_tidak_baku))
data

del(data['tokenize'])
data

data.to_csv('Hasil_Preprocessing.csv', encoding='utf8', index = False)

"""**WordCloud**"""

data = pd.read_csv('Hasil_Preprocessing.csv')
data

sentimen_count = data['sentimen'].value_counts()
sentimen_count

"""**WORCLOUD**"""

import matplotlib.pyplot as plt
from collections import Counter
from wordcloud import WordCloud, STOPWORDS

def plot_cloud (wordcloud):
    plt.figure(figsize=(10, 8))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.show()

all_words = ' '.join([tweets for tweets in data['stemming']])

wordcloud = WordCloud (
    width=3000,
    height=2000,
    random_state=3,
    background_color='black',
    colormap='RdPu',
    collocations=False,
    stopwords=STOPWORDS
).generate(all_words)
plot_cloud (wordcloud)

"""**PEMBOBOTAN KATA**"""

!pip install pip install scikit-learn

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

x_train, x_test, y_train, y_test = train_test_split(data['stemming'],data['sentimen'],test_size=0.2, stratify=data['sentimen'], random_state=42)

tvec = TfidfVectorizer()
clf = MultinomialNB()

"""**PEMODELAN**"""

from sklearn.pipeline import Pipeline

model = Pipeline([('vectorizer',tvec),('classifier',clf)])
model.fit(x_train,y_train)

hasil = model.predict(x_test)

"""**EVALUASI MODEL**"""

import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test,hasil)
print(cm)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',xticklabels=['negatif', 'netral', 'positif'], yticklabels=['negatif', 'netral', 'positif'])
plt.title('Confusion Matrix')
plt.xlabel('Kelas Prediksi')
plt.ylabel('Kelas Aktual')
plt.show()

matrix = classification_report(y_test,hasil)
accuracy = accuracy_score(y_test,hasil)
print(matrix)
print(accuracy)

# Inferensi dengan teks baru
new_texts = ["Pemilu Damai"]
new_predictions = model.predict(new_texts)

# Hasil prediksi
print(f"Prediksi Sentimen: {new_predictions[0]}")